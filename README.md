# Transformer Translator

In this project, I recreated and trained the Transformer architecture from the famous "Attention Is All You Need" paper using Pytorch to translate English to Spanish. 

# Parameters and other Variables 
| Configuration       | Value     |
|---------------------|-----------|
| Batch Size          | 64        |
| Sequence Length     | 96        |
| Max Iterations      | 50000     |
| Epochs              | 15        |
| Learning Rate       | 1e-5      |
| Embedding Dimension | 512       |
| Model Dimension     | 512       |
| Key Dimension (d_k) | 64        |
| Feedforward Dimension| 2048      |
| Number of Heads     | 8         |
| Head Size           | 64        |
| Number of Layers    | 6         |
| Dropout             | 0.2       |


